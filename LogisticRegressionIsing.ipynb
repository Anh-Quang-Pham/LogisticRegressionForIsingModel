{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use <code>train_test_split</code> to split training set further into training, test or validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=pd.read_csv('./IsingData/Ising40x40/xtrain.txt',sep='\\s+',header=None,\n",
    "                      skiprows=0,chunksize=2000,dtype=np.int32)\n",
    "x_train=np.array(x_train.get_chunk(2000))\n",
    "x_test=pd.read_csv('./IsingData/Ising40x40/xtest.txt',sep='\\s+',header=None,\n",
    "                   skiprows=0,chunksize=2000,dtype=np.int32)\n",
    "x_test=np.array(x_test.get_chunk(2000))                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=np.loadtxt('./IsingData/Ising40x40/ytrain.txt')\n",
    "y_test=np.loadtxt('./IsingData/Ising40x40/ytest.txt')\n",
    "\n",
    "y_train=y_train[:2000]\n",
    "y_test=y_test[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1600)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1, -1, ...,  1, -1, -1], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
       "       1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
       "       0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "       0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot Ising states\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# set colorbar map\n",
    "cmap_args=dict(cmap='plasma_r')\n",
    "\n",
    "L=40\n",
    "\n",
    "fig,axarr=plt.subplots(nrows=1,ncols=2)\n",
    "axarr[0].imshow(x_train[2].reshape(L,L),**cmap_args)\n",
    "axarr[0].set_title('$\\\\mathrm{ordered\\\\ phase}$',fontsize=16)\n",
    "axarr[0].tick_params(labelsize=16)\n",
    "\n",
    "axarr[1].imshow(x_train[1].reshape(L,L),**cmap_args)\n",
    "axarr[1].set_title('$\\\\mathrm{disordered\\\\ phase}$',fontsize=16)\n",
    "axarr[1].tick_params(labelsize=16)\n",
    "\n",
    "fig.subplots_adjust(right=2.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain shape is:(1600, 2000)\n",
      "ytrain shape is:(1, 2000)\n",
      "xtest shape is:(1600, 2000)\n",
      "ytest shape is:(1, 2000)\n"
     ]
    }
   ],
   "source": [
    "xtrain=x_train.reshape(x_train.shape[1],x_train.shape[0])\n",
    "ytrain=np.reshape(y_train,(1,y_train.shape[0]))\n",
    "xtest=x_test.reshape(x_test.shape[1],x_test.shape[0])\n",
    "ytest=np.reshape(y_test,(1,y_test.shape[0]))\n",
    "\n",
    "print(\"xtrain shape is:\"+str(xtrain.shape))\n",
    "print(\"ytrain shape is:\"+str(ytrain.shape))\n",
    "print(\"xtest shape is:\"+str(xtest.shape))\n",
    "print(\"ytest shape is:\"+str(ytest.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sigmoid function \n",
    "\n",
    "the variable z can be a scalar of an array (vector, matrix), the returned $s=\\sigma(z)$ will have the data format of input z\n",
    "\n",
    "$\\sigma(z)=\\sigma \\begin{bmatrix}\n",
    "       z_1    \\\\[0.3em]\n",
    "       z_2    \\\\[0.3em]\n",
    "       \\dots  \\\\[0.3em]\n",
    "       z_n\n",
    "     \\end{bmatrix}=\\begin{bmatrix}\n",
    "       \\frac{1}{1+e^{-z_1}}    \\\\[0.3em]\n",
    "       \\frac{1}{1+e^{-z_2}}     \\\\[0.3em]\n",
    "       \\dots  \\\\[0.3em]\n",
    "       \\frac{1}{1+e^{-z_n}} \n",
    "     \\end{bmatrix}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s=1/(1+np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeWithRandn(dim):\n",
    "    w=np.zeros((dim,1))\n",
    "    b=0\n",
    "    \n",
    "    assert(w.shape==(dim,1))\n",
    "    assert(isinstance(b,float) or isinstance(b,int))\n",
    "    \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activations:\n",
    "\n",
    "$A=\\sigma(W^TX+b)$\n",
    "\n",
    "cost function:\n",
    "\n",
    "$J = -\\frac{1}{m}\\sum_{i=1}^my^{(i)}\\log a^{(i)}+(1-y^{(i)})\\log(1-a^{(i)}$\n",
    "\n",
    "gradient to weight:\n",
    "\n",
    "$\\mathrm{d}w:=\\frac{\\partial J}{\\partial W}=\\frac{1}{m}X(A-Y)^T$\n",
    "\n",
    "gradient to bias:\n",
    "\n",
    "$\\mathrm{d}b:=\\frac{\\partial J}{\\partial b}=\\frac{1}{m}\\sum_{i=1}^m(a^{(i)}-y^{(i)})=\\frac{1}{m}\\mathrm{np.sum}(A-Y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w,b,X,Y):\n",
    "    m=X.shape[1]\n",
    "    \n",
    "    # forward propagation (X to cost)\n",
    "    A = sigmoid(np.dot(w.T,X)+b)\n",
    "    cost=-(np.dot(Y,np.log(A.T))+np.dot(np.log(1-A),(1-Y).T))/m\n",
    "    \n",
    "    # backward propagation \n",
    "    dw=np.dot(X,(A-Y).T)/m\n",
    "    db=np.sum(A-Y)/m\n",
    "    \n",
    "    assert(dw.shape==w.shape)\n",
    "    assert(db.dtype==float)\n",
    "    cost=np.squeeze(cost)\n",
    "    assert(cost.shape==())\n",
    "    \n",
    "    grads={\"dw\": dw,\n",
    "           \"db\": db}\n",
    "    \n",
    "    return grads,cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent\n",
    "def optimize(w,b,X,Y,num_iterations,learning_rate,print_cost=False):\n",
    "    costs=[]\n",
    "    for i in range(num_iterations):\n",
    "        grads,cost=propagate(w,b,X,Y) # compute cost, grads\n",
    "        dw=grads[\"dw\"]\n",
    "        db=grads[\"db\"]\n",
    "        \n",
    "        # update\n",
    "        w=w-learning_rate*dw\n",
    "        b=b-learning_rate*db\n",
    "        \n",
    "        # record the costs\n",
    "        if i%100==0:\n",
    "            costs.append(cost)\n",
    "        # print the cost every 100 training examples\n",
    "        if print_cost and i%100==0:\n",
    "            print(\"Cost after iteration %i: %f\" %(i,cost))\n",
    "            \n",
    "    params={\"w\":w,\n",
    "            \"b\":b}\n",
    "    grads={\"dw\":dw,\n",
    "           \"db\":db}\n",
    "        \n",
    "    return params,grads,costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,b,X):\n",
    "    m=X.shape[1]\n",
    "    Y_prediction=np.zeros((1,m))\n",
    "    w=w.reshape(X.shape[0],1)\n",
    "    A=sigmoid(np.dot(w.T,X)+b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        # convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        if A[0,i]<=0.5:\n",
    "            Y_prediction[0,i]=0\n",
    "        else:\n",
    "            Y_prediction[0,i]=1\n",
    "    \n",
    "    assert(Y_prediction.shape==(1,m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the defined modules to build a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate the defined modules into model\n",
    "def model(X_train,Y_train,X_test,Y_test,num_iterations=2000,learning_rate=0.5,print_cost=False):\n",
    "    # initialize parameters\n",
    "    w,b=initializeWithRandn(X_train.shape[0])\n",
    "    # gradient descent\n",
    "    parameters,grads,costs=optimize(w,b,X_train,Y_train,num_iterations,learning_rate,print_cost)\n",
    "    # retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w=parameters[\"w\"]\n",
    "    b=parameters[\"b\"]\n",
    "    # predict test/train set examples\n",
    "    Y_prediction_test=predict(w,b,X_test)\n",
    "    Y_prediction_train=predict(w,b,X_train)\n",
    "    # print train/test errors\n",
    "    print(\"train accuracy: {} %\".format(100-np.mean(np.abs(Y_prediction_train-Y_train))*100))\n",
    "    print(\"test accuracy: {} %\".format(100-np.mean(np.abs(Y_prediction_test-Y_test))*100))\n",
    "    \n",
    "    d={\"costs\":costs,\n",
    "       \"Y_prediction_test\":Y_prediction_test,\n",
    "       \"Y_prediction_train\":Y_prediction_train,\n",
    "       \"w\":w,\n",
    "       \"b\":b,\n",
    "       \"learning_rate\":learning_rate,\n",
    "       \"num_iterations\":num_iterations}\n",
    "    \n",
    "    return d\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train the model with 2000 epoches and learning rate 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.621908\n",
      "Cost after iteration 200: 0.580853\n",
      "Cost after iteration 300: 0.552773\n",
      "Cost after iteration 400: 0.531650\n",
      "Cost after iteration 500: 0.514808\n",
      "Cost after iteration 600: 0.500852\n",
      "Cost after iteration 700: 0.488966\n",
      "Cost after iteration 800: 0.478637\n",
      "Cost after iteration 900: 0.469515\n",
      "Cost after iteration 1000: 0.461359\n",
      "Cost after iteration 1100: 0.453988\n",
      "Cost after iteration 1200: 0.447271\n",
      "Cost after iteration 1300: 0.441102\n",
      "Cost after iteration 1400: 0.435403\n",
      "Cost after iteration 1500: 0.430107\n",
      "Cost after iteration 1600: 0.425163\n",
      "Cost after iteration 1700: 0.420527\n",
      "Cost after iteration 1800: 0.416164\n",
      "Cost after iteration 1900: 0.412044\n",
      "train accuracy: 83.95 %\n",
      "test accuracy: 48.25000000000001 %\n"
     ]
    }
   ],
   "source": [
    "d=model(xtrain,ytrain,xtest,ytest,num_iterations=2000,learning_rate=0.01,print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually 84% accuracy on our train set is not bad, since we build up our logistic regression from scratch. With tensorflow, and keras one can obtain a better optimization.\n",
    "\n",
    "Let us now try to use tensorflow and keras to improve our accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg=linear_model.LogisticRegression(C=1.0/0.1,\n",
    "                                      random_state=1,\n",
    "                                      verbose=0,\n",
    "                                      max_iter=1E3,\n",
    "                                      tol=1E-5,\n",
    "                                      solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phil-configs/Personal/pham/MyEnv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10.0, max_iter=1000.0, random_state=1, solver='sag',\n",
       "                   tol=1e-05)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6825"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(1,activation='sigmoid',input_shape=(1600,)))\n",
    "model.compile(optimizer=SGD(lr=0.01),loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "32/32 [==============================] - 4s 62ms/step - loss: 0.8668 - accuracy: 0.4903 - val_loss: 0.8354 - val_accuracy: 0.5135\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8712 - accuracy: 0.5088 - val_loss: 0.8633 - val_accuracy: 0.5075\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7607 - accuracy: 0.5690 - val_loss: 0.7900 - val_accuracy: 0.5100\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7880 - accuracy: 0.5438 - val_loss: 0.8460 - val_accuracy: 0.5055\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7031 - accuracy: 0.5893 - val_loss: 1.0715 - val_accuracy: 0.4955\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6780 - accuracy: 0.6047 - val_loss: 0.9701 - val_accuracy: 0.4925\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6878 - accuracy: 0.6256 - val_loss: 0.7790 - val_accuracy: 0.4950\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6697 - accuracy: 0.6067 - val_loss: 0.7706 - val_accuracy: 0.5050\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6226 - accuracy: 0.6370 - val_loss: 1.0877 - val_accuracy: 0.4905\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6331 - accuracy: 0.6673 - val_loss: 0.7807 - val_accuracy: 0.4990\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5972 - accuracy: 0.6726 - val_loss: 0.8570 - val_accuracy: 0.4930\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6278 - accuracy: 0.6613 - val_loss: 0.7864 - val_accuracy: 0.4970\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5622 - accuracy: 0.7185 - val_loss: 1.3924 - val_accuracy: 0.4800\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5756 - accuracy: 0.7238 - val_loss: 0.7924 - val_accuracy: 0.4925\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5664 - accuracy: 0.7046 - val_loss: 0.9812 - val_accuracy: 0.4815\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5464 - accuracy: 0.7251 - val_loss: 0.8033 - val_accuracy: 0.4920\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5482 - accuracy: 0.7386 - val_loss: 0.9559 - val_accuracy: 0.4765\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5333 - accuracy: 0.7046 - val_loss: 0.8481 - val_accuracy: 0.4695\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5742 - accuracy: 0.7055 - val_loss: 0.9853 - val_accuracy: 0.4635\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5071 - accuracy: 0.8078 - val_loss: 0.8175 - val_accuracy: 0.4875\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4973 - accuracy: 0.7836 - val_loss: 0.8313 - val_accuracy: 0.4785\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5075 - accuracy: 0.7433 - val_loss: 0.8391 - val_accuracy: 0.4750\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4775 - accuracy: 0.7854 - val_loss: 1.1753 - val_accuracy: 0.4670\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5551 - accuracy: 0.7061 - val_loss: 0.9231 - val_accuracy: 0.4595\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4691 - accuracy: 0.8200 - val_loss: 0.9620 - val_accuracy: 0.4545\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4779 - accuracy: 0.8004 - val_loss: 0.8450 - val_accuracy: 0.4945\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4518 - accuracy: 0.8086 - val_loss: 0.8313 - val_accuracy: 0.7050\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4256 - accuracy: 0.8962 - val_loss: 0.8339 - val_accuracy: 0.7070\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4362 - accuracy: 0.8503 - val_loss: 0.8522 - val_accuracy: 0.5230\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4371 - accuracy: 0.8488 - val_loss: 0.8461 - val_accuracy: 0.6970\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4264 - accuracy: 0.8652 - val_loss: 0.8579 - val_accuracy: 0.6735\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4280 - accuracy: 0.8469 - val_loss: 0.8618 - val_accuracy: 0.6875\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4131 - accuracy: 0.8880 - val_loss: 0.9980 - val_accuracy: 0.4470\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4169 - accuracy: 0.8602 - val_loss: 1.2887 - val_accuracy: 0.4425\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4404 - accuracy: 0.8738 - val_loss: 1.0263 - val_accuracy: 0.4515\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4384 - accuracy: 0.8203 - val_loss: 1.0337 - val_accuracy: 0.4475\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4420 - accuracy: 0.8179 - val_loss: 0.9322 - val_accuracy: 0.4595\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4140 - accuracy: 0.8143 - val_loss: 0.9062 - val_accuracy: 0.5045\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3842 - accuracy: 0.8758 - val_loss: 0.9645 - val_accuracy: 0.4545\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.3898 - accuracy: 0.8811 - val_loss: 0.9687 - val_accuracy: 0.4550\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4135 - accuracy: 0.8190 - val_loss: 0.9461 - val_accuracy: 0.4700\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3649 - accuracy: 0.9061 - val_loss: 1.0650 - val_accuracy: 0.4380\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3771 - accuracy: 0.9053 - val_loss: 0.9237 - val_accuracy: 0.6715\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3952 - accuracy: 0.8320 - val_loss: 0.9233 - val_accuracy: 0.6840\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3704 - accuracy: 0.8957 - val_loss: 0.9932 - val_accuracy: 0.4525\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3788 - accuracy: 0.8473 - val_loss: 0.9257 - val_accuracy: 0.6925\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.3435 - accuracy: 0.9458 - val_loss: 0.9254 - val_accuracy: 0.6995\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3440 - accuracy: 0.9251 - val_loss: 0.9995 - val_accuracy: 0.4635\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.3702 - accuracy: 0.8490 - val_loss: 0.9419 - val_accuracy: 0.6915\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.3358 - accuracy: 0.9420 - val_loss: 0.9787 - val_accuracy: 0.5015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa8bc0ae280>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=50,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, with keras we obtain a better accuracy for our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
